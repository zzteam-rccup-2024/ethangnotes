{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Course 4: Mediapipe\n",
    "\n",
    "`Mediapipe` is a framework for building multimodal (e.g. video, audio, any time series data), cross-platform (i.e. Android, iOS, web, edge devices) applied ML pipelines. It is used for research and production applications.\n",
    "\n",
    "In this notebook, we will use `mediapipe` library to recognise the gesture, and train a model to recognise rock, paper and scissors gestures.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to install the `mediapipe` library.\n",
    "\n",
    "```bash\n",
    "pip install mediapipe\n",
    "```\n",
    "\n",
    "Note that `mediapipe` requires `opencv-python` to be installed.\n",
    "\n",
    "It is worthwhile to announce that in `OpenCV`, the default color space is `BGR`. So firstly, we need to convert the image to `RGB` color space.\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "dest = cv2.cvtColor(src, cv2.COLOR_BGR2RGB)\n",
    "```\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "`Mediapipe` provides a variety of models for different tasks. For example, `Hands` model is used to detect hands and hand landmarks. `Pose` model is used to detect human poses. `Face` model is used to detect faces and facial landmarks. Take `Hands` as an example, we can recognize nodes (about 21) of a hand."
   ],
   "id": "e4c92cea8bfd343a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-16T12:56:24.608955Z",
     "start_time": "2024-07-16T12:55:59.269045Z"
    }
   },
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "image = cv2.imread('./mediapipe/victory.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(image)\n",
    "\n",
    "h, w, c = image.shape\n",
    "\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "cv2.imshow('Hand Tracking', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "hands.close()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1721134559.276552  404695 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n",
      "W0000 00:00:1721134559.283286  445423 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1721134559.289613  445423 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_RGB2BGR)\n\u001B[1;32m     22\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHand Tracking\u001B[39m\u001B[38;5;124m'\u001B[39m, image)\n\u001B[0;32m---> 23\u001B[0m cv2\u001B[38;5;241m.\u001B[39mwaitKey(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     24\u001B[0m cv2\u001B[38;5;241m.\u001B[39mdestroyAllWindows()\n\u001B[1;32m     26\u001B[0m hands\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chore: integrate the camera and recognize hands promptly.",
   "id": "80529a31ac58bd58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:30:23.359469Z",
     "start_time": "2024-07-16T11:30:15.210056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    ret, image = camera.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    h, w, c = image.shape\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imshow('Hand Tracking', image)\n",
    "    if cv2.waitKey(1) and 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "hands.close()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "4ef558d6b574b87a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1721129416.564226  280736 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n",
      "W0000 00:00:1721129416.569112  339687 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1721129416.575007  339687 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m mp_draw \u001B[38;5;241m=\u001B[39m mp\u001B[38;5;241m.\u001B[39msolutions\u001B[38;5;241m.\u001B[39mdrawing_utils\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m----> 9\u001B[0m     ret, image \u001B[38;5;241m=\u001B[39m camera\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gesture Recognition based on rules\n",
    "\n",
    "- `results.multi_hand_landmarks` is a list containing two left and right hand landmark values.  \n",
    "Each of these elements can be accessed point by point using `landmark.landmark`\n",
    "- `mcp` metacarpophalangeal joint\n",
    "- `ip` interphalangeal joint\n",
    "- `pip` proximal interphalangeal joint\n",
    "- `dip` distal interphalangeal joint\n",
    "- `cmc` carpometacarpal joint\n",
    "\n",
    "> Because its implementation is too rude, I just adapted from the source and did not modify it."
   ],
   "id": "677edf739b74b48f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "image = cv2.imread('./mediapipe/victory.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(image)\n",
    "h, w, c = image.shape\n",
    "\n",
    "lst_lms = []\n",
    "\n",
    "# Add all gesture coordinate points to a list [(x1, y1), (x2, y2),....] Total 21\n",
    "if results.multi_hand_landmarks:\n",
    "    for single_hand_marks in results.multi_hand_landmarks:\n",
    "        for id, lm in enumerate(single_hand_marks.landmark):\n",
    "            # The original `lm.x` and `lm.y` are decimals, multiplied by `w` and `h` to make the true coordinates\n",
    "            x, y = int(w * lm.x), int(h * lm.y)\n",
    "            cv2.circle(image, (x, y), 2, (255, 1, 0), -1)\n",
    "            lst_lms.append([x, y])\n",
    "\n",
    "lst_lms = np.array(lst_lms)\n",
    "hull_index = [0, 1, 2, 3, 6, 10, 14, 18, 17]  # Taking out nine points of the incoming target.\n",
    "hull = cv2.convexHull(lst_lms[hull_index, :])  # Connecting the nine points into a closed loop.\n",
    "\n",
    "cv2.polylines(image, [hull], True, (222, 222, 0), 2)  # Draw out this closed loop\n",
    "\n",
    "up_finger = []  # Here's the list outside the closed loop\n",
    "\n",
    "# Rotate these five fingertip points to see which ones are outside the closed loop of the above\n",
    "for i in [4, 8, 12, 16, 20]:\n",
    "    point = (int(lst_lms[i][0]), int(lst_lms[i][1]))\n",
    "    # Calculates the distance from the point to the outline, less than 0 means outside the outline.\n",
    "    dist = cv2.pointPolygonTest(hull, point, True)\n",
    "    print(dist)\n",
    "    if dist < 0:\n",
    "        up_finger.append(i)\n",
    "print(up_finger)\n",
    "\n",
    "if len(up_finger) == 1 and up_finger[\n",
    "    0] == 8:  # If there is only one point outside the closed loop and this hand is point 8\n",
    "    guesture = 'one'\n",
    "else:\n",
    "    guesture = 'None'\n",
    "\n",
    "if guesture:\n",
    "    cv2.putText(image, guesture, (30, 30), cv2.FONT_HERSHEY_COMPLEX, 1, (222, 21, 122), 1)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "cv2.imshow('MediaPipe Hand Tracking', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()"
   ],
   "id": "4e89accbb92f6cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gesture Recognition based on Machine Learning\n",
    "\n",
    "Via `PyTorch` and some datasets, we can easily recognise the gestures of rock, paper and scissors.\n",
    "\n",
    "We set the classifier as 4 results, including rock, paper, scissors and none.\n",
    "\n",
    "Unfortunately, official `mediapipe` supports TensorFlow (made by Google) but not PyTorch, so we can't use the bridge library (`mediapipe_model_maker`), and we need to implement the model by ourselves.\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "We first perform the recognition via `mediapipe` and then use the coordinates of the hand landmarks as the input of the model, returning them as $(\\mathrm{id}, x, y)$. Then we can use these coordinates to train the model."
   ],
   "id": "ffdef1b936fbeb79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:37:56.956668Z",
     "start_time": "2024-07-16T13:37:54.067989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from cv2.typing import MatLike\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "class ToNumpy:\n",
    "    def __call__(self, image: Image):\n",
    "        return np.array(image)\n",
    "\n",
    "class CropToHand:\n",
    "    def __call__(self, image: MatLike):\n",
    "        global y_min, y_max, x_min, x_max, hand_image_resized\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Get the bounding box coordinates of the hand\n",
    "                image_height, image_width, _ = image.shape\n",
    "                x_min = image_width\n",
    "                y_min = image_height\n",
    "                x_max = y_max = 0\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x = int(landmark.x * image_width)\n",
    "                    y = int(landmark.y * image_height)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "    \n",
    "            # Crop the hand region from the frame\n",
    "            hand_image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Resize the cropped hand image to 256x256\n",
    "            hand_image_resized = cv2.resize(hand_image, (256, 256))\n",
    "            return hand_image_resized\n",
    "        else:\n",
    "            return image.reshape((256, 256))\n",
    "\n",
    "class ExtractKeypoints:\n",
    "    def __init__(self, hands: mp_hands.Hands):\n",
    "        self.hands = hands\n",
    "    \n",
    "    def __call__(self, image: MatLike) -> np.ndarray:\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        results = hands.process(image)\n",
    "        h, w, c = image.shape\n",
    "        lst_lms = []\n",
    "        x0, y0 = 0, 0\n",
    "        if results.multi_hand_landmarks:\n",
    "            for single_hand_marks in results.multi_hand_landmarks:\n",
    "                for id, lm in enumerate(single_hand_marks.landmark):\n",
    "                    if id == 0:\n",
    "                        x0, y0 = int(w * lm.x), int(h * lm.y)\n",
    "                    else:\n",
    "                        x, y = int(w * lm.x) - x0, int(h * lm.y) - y0\n",
    "                        lst_lms.append([id, x, y])\n",
    "        \n",
    "        return np.array(lst_lms, dtype=np.float32)\n",
    "\n",
    "class HandleGestureDataset:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, matrix: np.ndarray):\n",
    "        # assert matrix.shape == (20, 3)\n",
    "        if matrix.shape == (0, 0) or matrix.shape == (0, ):\n",
    "            matrix = np.random.random((5, 4, 2))\n",
    "            return matrix\n",
    "        # Find the missed points\n",
    "        if matrix.shape[0] < 20:\n",
    "            print(matrix, matrix.shape)\n",
    "            for i in range(1, 21):\n",
    "                if i not in matrix[:, 0]:\n",
    "                    matrix = np.insert(matrix, i, [i, 0, 0], axis=0)\n",
    "        matrix = matrix[:, 1:]\n",
    "        matrix = matrix.reshape(5, 4, 2)\n",
    "        return matrix\n",
    "\n",
    "image = cv2.imread('./mediapipe/victory.jpg')\n",
    "\n",
    "points = ExtractKeypoints(hands)(image)\n",
    "handler = HandleGestureDataset()\n",
    "handler(points)"
   ],
   "id": "67fdf55d3f344d9e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721137076.931580  485802 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1721137076.935914  486091 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1721137076.940586  486091 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.83560001, 0.6741797 ],\n",
       "        [0.71665594, 0.60959557],\n",
       "        [0.92752786, 0.87106068],\n",
       "        [0.12921649, 0.04644386]],\n",
       "\n",
       "       [[0.41396106, 0.35537373],\n",
       "        [0.99849717, 0.3019162 ],\n",
       "        [0.15953014, 0.96268324],\n",
       "        [0.02818244, 0.20892876]],\n",
       "\n",
       "       [[0.28579477, 0.11263638],\n",
       "        [0.14204413, 0.4199289 ],\n",
       "        [0.3806742 , 0.37125793],\n",
       "        [0.98116345, 0.65386357]],\n",
       "\n",
       "       [[0.25299978, 0.2490395 ],\n",
       "        [0.60256314, 0.51773068],\n",
       "        [0.37269369, 0.4607935 ],\n",
       "        [0.88227983, 0.31962578]],\n",
       "\n",
       "       [[0.68383068, 0.20295217],\n",
       "        [0.917635  , 0.48825266],\n",
       "        [0.34977718, 0.42750554],\n",
       "        [0.25990254, 0.83325976]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Datasets\n",
    "\n",
    "We can use the `rock_paper_scissors` dataset (downloaded to ./mediapipe/rps folder) to train the model. Via `ImageFolder`, these images can be loaded easily."
   ],
   "id": "624554587815b810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:01.220511Z",
     "start_time": "2024-07-16T13:38:00.207324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.RandomRotation((-90, 90)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    ToNumpy(),\n",
    "    ExtractKeypoints(hands),\n",
    "    HandleGestureDataset(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder('./mediapipe/rps', transform=data_transform)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "6dbf2d36dca4506b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network Design\n",
    "\n",
    "Inspired by convolutional neural networks, we firstly use convolutional layers to extract features, and then use fully connected layers to classify the gestures. Finally, we use the softmax function to output the probabilities of each gesture."
   ],
   "id": "a35ef434a82bbaad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:02.743467Z",
     "start_time": "2024-07-16T13:38:02.733811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GestureClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GestureClassifier, self).__init__()\n",
    "        # Define convolution and pooling layers\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, padding=1, stride=1)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(40, 64)  # Adjusted to match the output shape after conv and pooling\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        slices = []\n",
    "\n",
    "        # Loop over the 5 slices in the input\n",
    "        for i in range(5):\n",
    "            slice = x[:, :, i, :].unsqueeze(1)  # Extract the i-th slice and add channel dimension\n",
    "            conv_out = self.conv(slice)\n",
    "            pool_out = self.pooling(conv_out)\n",
    "            slices.append(pool_out)\n",
    "\n",
    "        # Stack the slices and flatten\n",
    "        x = torch.cat(slices, dim=1)  # Concatenate slices along the channel dimension\n",
    "        x = x.view(batch_size, -1)  # Flatten to (batch_size, 5 * 3 * 3)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = GestureClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10"
   ],
   "id": "7886a6f497463992",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "We can train the model via the following code."
   ],
   "id": "c81ad4e353ae0431"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:05.610750Z",
     "start_time": "2024-07-16T13:38:05.603573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_model = False\n",
    "if train_model:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for (data, target) in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "            data, target = data.float().to(device), target.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item() / batch_size}')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (data, target) in tqdm(val_loader):\n",
    "                data, target = data.float().to(device), target.float().to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "            \n",
    "            accuracy = correct / total\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Accuracy: {accuracy}')\n",
    "    \n",
    "    torch.save(model.state_dict(), './mediapipe/gesture_classifier.pth')\n",
    "else:\n",
    "    model.load_state_dict(torch.load('./mediapipe/gesture_classifier.pth'))"
   ],
   "id": "99ca49da243594ab",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "We can call the camera and evaluate the model."
   ],
   "id": "a2f095fdc6706e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:16.039889Z",
     "start_time": "2024-07-16T13:38:08.261262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def have_hand(image: MatLike):\n",
    "    results = hands.process(image)\n",
    "    if results.multi_hand_landmarks:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "while True:\n",
    "    ret, image = camera.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transformer = transforms.Compose([\n",
    "        ExtractKeypoints(hands),\n",
    "        HandleGestureDataset(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    if have_hand(image):\n",
    "        src = transformer(image).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(src.unsqueeze(0))\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            print(predicted.item())\n",
    "        cv2.putText(image, str(predicted.item()), (90, 90), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 1)\n",
    "    cv2.putText(image, str(have_hand(image)), (30, 30), cv2.FONT_HERSHEY_COMPLEX, 1, (222, 21, 122), 1)\n",
    "\n",
    "    cv2.imshow('Hand Tracking', image)\n",
    "    if cv2.waitKey(1) and 0xFF == ord('q'):\n",
    "        break"
   ],
   "id": "919907417e295bb0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1721137088.266770  485802 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n",
      "W0000 00:00:1721137088.271231  486500 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1721137088.275946  486504 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/opt/anaconda3/envs/training/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 33\u001B[0m\n\u001B[1;32m     30\u001B[0m cv2\u001B[38;5;241m.\u001B[39mputText(image, \u001B[38;5;28mstr\u001B[39m(have_hand(image)), (\u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m30\u001B[39m), cv2\u001B[38;5;241m.\u001B[39mFONT_HERSHEY_COMPLEX, \u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m222\u001B[39m, \u001B[38;5;241m21\u001B[39m, \u001B[38;5;241m122\u001B[39m), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     32\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHand Tracking\u001B[39m\u001B[38;5;124m'\u001B[39m, image)\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cv2\u001B[38;5;241m.\u001B[39mwaitKey(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;241m0xFF\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mord\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mq\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Final Game: Play Rock, Paper, Scissors with the Computer!\n",
    "\n",
    "Via `random`, we can play the game with the computer.\n",
    "\n",
    "1. Computer set the countdown;\n",
    "2. Random the computer's action (rock, paper, scissors);\n",
    "3. Recognize the player's action;\n",
    "4. Compare the results.\n",
    "\n",
    "Let's begin.\n",
    "\n",
    "#### 1. Computer set the countdown"
   ],
   "id": "db961647a180e661"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:23.262575Z",
     "start_time": "2024-07-16T13:38:20.154275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "for i in range(3, 0, -1):\n",
    "    print(i)\n",
    "    time.sleep(1)\n",
    "\n",
    "print('Go!')\n",
    "\n",
    "time.sleep(0.1)"
   ],
   "id": "85afe40d12c6df22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n",
      "Go!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2. Random the computer's action\n",
    "\n",
    "We can use `random` to random the computer's action."
   ],
   "id": "aafbbfc0717dc433"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:26.488535Z",
     "start_time": "2024-07-16T13:38:26.486246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "actions = ['paper', 'rock', 'scissors']\n",
    "\n",
    "computer_action = random.choice(actions)"
   ],
   "id": "6120d776e896e23c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3. Recognize the player's action\n",
    "\n",
    "Get 30 frames to recognize the player's action, it can reduce the risk of misjudgment."
   ],
   "id": "724416ce6885d54f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:47.786099Z",
     "start_time": "2024-07-16T13:38:46.296415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "player_action = None\n",
    "record = [0, 0, 0, 0]\n",
    "\n",
    "for _ in range(30):\n",
    "    ret, image = camera.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transformer = transforms.Compose([\n",
    "        ExtractKeypoints(hands),\n",
    "        HandleGestureDataset(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    if have_hand(image):\n",
    "        src = transformer(image).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(src.unsqueeze(0))\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            record[predicted.item()] += 1\n",
    "\n",
    "if max(record) == record[0]:\n",
    "    player_action = 'none'\n",
    "elif max(record) == record[1]:\n",
    "    player_action = 'paper'\n",
    "elif max(record) == record[2]:\n",
    "    player_action = 'rock'\n",
    "else:\n",
    "    player_action = 'scissors'\n",
    "\n",
    "print(f\"Player's action: {player_action}, Computer's action: {computer_action}\")"
   ],
   "id": "740097e59700d339",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player's action: paper, Computer's action: scissors\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4. Compare the results",
   "id": "483aab5f28fbae0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T13:38:50.130114Z",
     "start_time": "2024-07-16T13:38:50.126268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if player_action == 'none':\n",
    "    print('You did not make any action.')\n",
    "elif player_action == computer_action:\n",
    "    print('Draw!')\n",
    "elif (player_action == 'rock' and computer_action == 'scissors') or (player_action == 'scissors' and computer_action == 'paper') or (player_action == 'paper' and computer_action == 'rock'):\n",
    "    print('You win!')\n",
    "else:\n",
    "    print('You lose!')"
   ],
   "id": "e8a5a794bc583ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You lose!\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
